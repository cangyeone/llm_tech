# 教程：RLHF、DPO、KTO 对齐方法对比

## 学习目标
- 明确三种主流对齐技术的优化流程、数据需求与优缺点。
- 学会将抽象概念转化为可视化表格与雷达图，辅助课堂讲解。
- 理解不同方法在实现复杂度、样本效率、算力需求等维度的差异。

## 背景原理
- **RLHF**：通过奖励模型与强化学习（常用 PPO）最大化期望奖励：$\max_{\pi} \mathbb{E}_{y \sim \pi} [r(y)]$。
- **DPO**：利用偏好对直接优化策略，损失函数形式：

$$
\mathcal{L}_{\text{DPO}} = -\log \sigma\left(\beta [\log \pi_{\theta}(y^+|x) - \log \pi_{\theta}(y^-|x)] - [\log \pi_{\text{ref}}(y^+|x) - \log \pi_{\text{ref}}(y^-|x)]\right).
$$

- **KTO**：在偏好缺失场景下，通过加权损失统一处理正负反馈，兼容“无偏好”标注。

## 代码结构解析
- `AlignmentMethod` 数据类：描述每种方法的关键属性与雷达图评分。
- `to_markdown`：生成 Markdown 表格，便于插入课程文档。
- `plot_radar`：使用 matplotlib 绘制雷达图，展示多维指标差异。
- `main`：打印表格并尝试绘制雷达图，若图形后端不可用则给出提示。

## 实践步骤
1. 运行脚本，复制输出的 Markdown 表格到教学 PPT 或笔记中。
2. 若环境支持 GUI，将展示雷达图，可用于课堂讨论。
3. 根据实际经验调整 `score_profile`，使评估更贴近团队场景。

## 延伸思考
- RLHF 的奖励模型可以使用偏好数据训练，也可引入 AI 反馈，如何影响 `score_profile`？
- DPO 在低质量偏好对上可能过拟合，是否需要温度参数 $\beta$ 调整？
- KTO 兼容无偏好样本，对标注流程有哪些启发？
