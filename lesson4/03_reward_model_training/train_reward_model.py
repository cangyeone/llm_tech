from __future__ import annotations

import argparse
from dataclasses import dataclass
from typing import List

import evaluate
import numpy as np
import torch
from datasets import Dataset, load_dataset, get_dataset_split_names
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments
from transformers import (
    AutoConfig,
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
)
import torch.nn as nn 
# --- å¸¸é‡å®šä¹‰ ---
DEFAULT_MODEL_NAME = "Qwen/Qwen3-0.6b" 
DEFAULT_DATASET = "lvwerra/stack-exchange-paired"


@dataclass
class ScriptArguments:
    model_name: str = DEFAULT_MODEL_NAME
    dataset_name: str = DEFAULT_DATASET
    subset: str | None = None
    output_dir: str = "./outputs/reward_model"
    learning_rate: float = 5e-6
    num_train_epochs: int = 1
    per_device_batch_size: int = 2
    max_length: int = 512
    fraction: float = 0.01 # é»˜è®¤åªå– 1% æ•°æ®è¿›è¡Œå¿«é€Ÿå®éªŒ


# --- æ•°æ®åŠ è½½å’Œå¤„ç† ---

def load_preference_dataset(args: ScriptArguments) -> tuple[Dataset, Dataset]:
    name   = args.dataset_name
    frac   = getattr(args, "fraction", 0.01)
    frac_str = "" if frac >= 1.0 else f"[:1%]"

    # ğŸš€ ä¿®æ­£ 1ï¼šæŒ‡å®šæ•°æ®å­ç›®å½• (stack-exchange-paired æ•°æ®é›†éœ€è¦è¿™ä¸ª)
    ds_train = load_dataset(name, data_dir='data/finetune', split=f"train{frac_str}")
    # æ³¨æ„ï¼šè¿™ä¸ªæ•°æ®é›†é»˜è®¤åªæœ‰ train splitï¼Œæˆ‘ä»¬ä» train split æ‹†åˆ†å‡ºè¯„ä¼°é›†
    
    # ç®€å•çš„ train/test æ‹†åˆ† (80/20)
    ds_split = ds_train.train_test_split(test_size=0.2, seed=42)
    ds_train = ds_split["train"]
    ds_test  = ds_split["test"]

    # ç»Ÿä¸€æˆ {prompt, chosen, rejected}
    def to_pref(ex):
        # lvwerra/stack-exchange-paired çš„å­—æ®µ
        
        # 1. ç¡®è®¤å­—æ®µå­˜åœ¨ï¼šä½ åº”è¯¥æ£€æŸ¥æ•°æ®é›†çš„åŸå§‹å­—æ®µåã€‚
        # æ ¹æ®æ•°æ®é›†è®¾è®¡ï¼Œresponse_j åº”è¯¥å¯¹åº” chosen (æ›´å¥½çš„å›ç­”)ï¼Œ
        # response_k åº”è¯¥å¯¹åº” rejected (è¾ƒå·®çš„å›ç­”)ã€‚
        
        # ç¡®ä¿å­—æ®µå­˜åœ¨
        if "question" not in ex or "response_j" not in ex or "response_k" not in ex:
            # å¦‚æœä½ çœ‹åˆ°äº†è¿™ä¸ªé”™è¯¯ï¼Œè¯´æ˜åŠ è½½çš„æ•°æ®é›†ç»“æ„ä¸æ­£ç¡®
            raise KeyError("åŠ è½½çš„æ•°æ®é›†ç¼ºå°‘ 'question', 'response_j', æˆ– 'response_k' å­—æ®µã€‚")
            
        prompt = ex["question"]
        chosen = ex["response_j"]
        rejected = ex["response_k"]
        
        
        return {"prompt": prompt, "chosen": chosen, "rejected": rejected}


    ds_train = ds_train.map(to_pref, remove_columns=ds_train.column_names)
    ds_test  = ds_test.map(to_pref,  remove_columns=ds_test.column_names)
    return ds_train, ds_test


# ==== 1) é¢„å¤„ç†ï¼šä¸²æ¥ prompt+answerï¼Œè¿”å› (B,2,L) ====
def preprocess(dataset: Dataset, tokenizer: AutoTokenizer, max_length: int) -> Dataset:
    def _encode(batch):
        prompts  = [f"é—®é¢˜ï¼š{q}\nå›ç­”ï¼š" for q in batch["prompt"]]
        chosens  = batch["chosen"]
        rejecteds= batch["rejected"]

        seq_chosen  = [p + c for p, c in zip(prompts, chosens)]
        seq_rejected= [p + r for p, r in zip(prompts, rejecteds)]

        enc_c = tokenizer(seq_chosen,  truncation=True, padding="max_length", max_length=max_length)
        enc_r = tokenizer(seq_rejected,truncation=True, padding="max_length", max_length=max_length)

        input_ids      = np.stack([enc_c["input_ids"],      enc_r["input_ids"]], axis=1)   # (B,2,L)
        attention_mask = np.stack([enc_c["attention_mask"], enc_r["attention_mask"]], axis=1)
        return {"input_ids": input_ids, "attention_mask": attention_mask}

    return dataset.map(_encode, batched=True, remove_columns=dataset.column_names)


# --- æŸå¤±å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡ ---

def compute_loss(self, model, inputs, return_outputs=False):
    # âš ï¸ 1. ç¡®ä¿åªæå– input_ids å’Œ attention_mask 
    # å¹¶ä¸”å°†å®ƒä»¬è½¬ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡ (å¦‚æœæ¨¡å‹ä¸åœ¨ CPU ä¸Š)
    device = model.device
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    # âš ï¸ 2. æ£€æŸ¥ attention_mask çš„ Dtype: 
    # Qwen3 æ¨¡å‹å¯èƒ½æœŸæœ› attention_mask æ˜¯ LongTensor (0å’Œ1)
    # ç¡®ä¿å®ƒä¸æ˜¯æµ®ç‚¹æ•°ï¼Œä¸”ä¸æ˜¯ä½ æ„å¤–ä¼ å…¥çš„ input_idsã€‚
    if attention_mask.dtype != torch.long:
        attention_mask = attention_mask.long()

    # å°† (batch_size, 2, seq_len) reshape ä¸º (batch_size * 2, seq_len) 
    outputs = model(
        input_ids=input_ids.view(-1, input_ids.size(-1)),
        attention_mask=attention_mask.view(-1, attention_mask.size(-1)),
        # ç§»é™¤æ‰€æœ‰å…¶ä»–å¯èƒ½çš„å‚æ•°ï¼Œå¦‚ token_type_ids, labels ç­‰ï¼Œé™¤éä½ æ˜ç¡®éœ€è¦
    )
    
    # 3. æŸå¤±è®¡ç®—
    logits = outputs.logits.view(-1, 2) 
    loss = -torch.nn.functional.logsigmoid(logits[:, 0] - logits[:, 1]).mean()
    
    return (loss, outputs) if return_outputs else loss


# ==== 4) metricsï¼šç”¨ (chosen - rejected) ä½œä¸ºé¢„æµ‹åˆ†å·® ====
import evaluate, numpy as np, torch

def compute_metrics(eval_pred):
    # æ¥è‡ª Trainer çš„ predictions å¯èƒ½æ˜¯ Noneï¼Œè¿™é‡Œä» inputs é‡Œé‡ç®—æ›´ç¨³å¦¥ã€‚
    # ä¸ºç®€æ´ï¼Œè¿™é‡Œå‡è®¾æˆ‘ä»¬åœ¨ evaluation æ—¶å¤ç”¨ compute_loss çš„é€»è¾‘ï¼š
    # ç›´æ¥è®© Trainer è¿”å› predictions=scores_diffï¼ˆè§ä¸‹æ–¹ make_trainerï¼‰ã€‚
    diff = np.array(eval_pred.predictions)  # shape: (N,)
    win_rate = float((diff > 0).mean())
    kendall = evaluate.load("kendalltau")
    tau = kendall.compute(predictions=diff, references=np.ones_like(diff), variant="b")["kendalltau"]
    return {"win_rate": win_rate, "kendall_tau": tau}


def _rope_full_dim_config(model_name: str):
    cfg = AutoConfig.from_pretrained(model_name, trust_remote_code=True)
    # ä»¤ rotary_dim = head_dimï¼Œç¦ç”¨ partial/scalingï¼Œé¿å… RoPE ç»´åº¦é”™é…
    head_dim = cfg.hidden_size // cfg.num_attention_heads
    setattr(cfg, "rotary_dim", head_dim)
    if hasattr(cfg, "partial_rotary_factor"): cfg.partial_rotary_factor = 1.0
    if hasattr(cfg, "rope_scaling"): cfg.rope_scaling = None
    if hasattr(cfg, "use_dynamic_ntk"): cfg.use_dynamic_ntk = False
    if hasattr(cfg, "rope_theta_1"): cfg.rope_theta_1 = None
    if hasattr(cfg, "num_key_value_heads") and cfg.num_key_value_heads is None:
        cfg.num_key_value_heads = cfg.num_attention_heads
    return cfg

# ==== 2) LM + Value Headï¼šä»…è¿”å› valuesï¼ˆåºåˆ—æ¯ä¸ªä½ç½®çš„å€¼ï¼‰ï¼Œä¸è¿”å› LM logits ====
class CausalLMWithValueHead(nn.Module):
    def __init__(self, base: AutoModelForCausalLM):
        super().__init__()
        self.base = base
        hidden = base.config.hidden_size
        self.value_head = nn.Linear(hidden, 1)
        nn.init.normal_(self.value_head.weight, std=1e-2)
        nn.init.zeros_(self.value_head.bias)

    @property
    def config(self): 
        return self.base.config

    def forward(self, input_ids, attention_mask=None, output_hidden_states=True):
        out = self.base(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=output_hidden_states,
            use_cache=False,
        )
        h = out.hidden_states[-1]             # [B, L, H]
        values = self.value_head(h).squeeze(-1)  # [B, L]
        return {"values": values}             # åªæš´éœ²åºåˆ—æ¯ä¸ª token çš„ value

# --- Trainer å°è£… ---
# ==== 3) è‡ªå®šä¹‰ Trainerï¼šå®ç° pairwise æŸå¤± ====
class PairwiseRewardTrainer(Trainer):
    # å…¼å®¹æ–°ç‰ˆæœ¬å¤šçš„å‚æ•°
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        # (B, 2, L) â€”â€” å·²ç»åœ¨æ­£ç¡®è®¾å¤‡ä¸Šäº†
        input_ids = inputs["input_ids"]              # tensor, device å·²æ­£ç¡®
        attention_mask = inputs["attention_mask"]    # tensor, device å·²æ­£ç¡®
        if attention_mask.dtype != torch.long:
            attention_mask = attention_mask.long()

        B, P, L = input_ids.shape  # P=2
        input_ids = input_ids.view(B * P, L)
        attention_mask = attention_mask.view(B * P, L)

        out = model(input_ids=input_ids, attention_mask=attention_mask)  # {"values": [B*P, L]}
        values = out["values"]

        # ç”¨æœ€åä¸€ä¸ªé PAD ä½ç½®çš„ value ä½œä¸ºåºåˆ—åˆ†æ•°
        last_idx = attention_mask.sum(dim=-1) - 1            # (B*P,)
        seq_score = values.gather(1, last_idx.unsqueeze(1)).squeeze(1)  # (B*P,)

        scores = seq_score.view(B, 2)    # (B, 2) -> [chosen, rejected]
        chosen, rejected = scores[:, 0], scores[:, 1]

        loss = -torch.nn.functional.logsigmoid(chosen - rejected).mean()
        return (loss, out) if return_outputs else loss



import os 
# ==== 5) ç»„è£… Trainerï¼šå»æ‰ RoPE æ”¹å†™ï¼Œå¼€å¯è¯„æµ‹ ====
def make_trainer(args: ScriptArguments, tokenized_train: Dataset, tokenized_test: Dataset, tokenizer: AutoTokenizer) -> Trainer:
    base = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        attn_implementation="eager",
        trust_remote_code=True,
    )
    model = CausalLMWithValueHead(base)
    model.base.resize_token_embeddings(len(tokenizer))

    training_args = TrainingArguments(
        output_dir=args.output_dir,
        per_device_train_batch_size=args.per_device_batch_size,
        num_train_epochs=args.num_train_epochs,
        learning_rate=args.learning_rate,
        fp16=torch.cuda.is_available(),
        logging_steps=20,
        eval_steps=100,
        save_strategy="steps",         # âœ… æŒ‰æ­¥ä¿å­˜
        save_steps=10,   
        save_safetensors=False, 
        remove_unused_columns=False,
    )

    # è®© Trainer åœ¨è¯„æµ‹æ—¶è¾“å‡º predictions=score_diffï¼Œä¾¿äº compute_metrics
    class EvalPairwiseTrainer(PairwiseRewardTrainer):
        def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):
            with torch.no_grad():
                device = model.device
                input_ids = torch.as_tensor(inputs["input_ids"], device=device)
                attention_mask = torch.as_tensor(inputs["attention_mask"], device=device).long()
                B, P, L = input_ids.shape
                input_ids = input_ids.view(B*P, L)
                attention_mask = attention_mask.view(B*P, L)
                out = model(input_ids=input_ids, attention_mask=attention_mask)
                values = out["values"]
                last_idx = attention_mask.sum(dim=-1) - 1
                seq_score = values.gather(1, last_idx.unsqueeze(1)).squeeze(1)   # (B*P,)
                scores = seq_score.view(B, 2)
                chosen, rejected = scores[:, 0], scores[:, 1]
                diff = (chosen - rejected).detach().float().cpu().numpy()        # (B,)
                loss = -torch.nn.functional.logsigmoid(chosen - rejected).mean().detach().cpu()
            return (loss, diff, None)
        def _save(self, output_dir: str, state_dict=None):
            os.makedirs(output_dir, exist_ok=True)
            m = self.model
            # è‹¥æ˜¯æˆ‘ä»¬åŒ…è£…çš„ CausalLMWithValueHead
            if hasattr(m, "base") and hasattr(m, "value_head"):
                # 1) ä¿å­˜åŸºåº§ï¼šHF è‡ªå·±å¤„ç† tied weights
                m.base.save_pretrained(output_dir)
                # 2) ä¿å­˜ value_head
                vh_path = os.path.join(output_dir, "value_head.pt")
                torch.save(m.value_head.state_dict(), vh_path)
                # 3) ä¿å­˜ tokenizerï¼ˆè‹¥å¯ç”¨ï¼‰
                if getattr(self, "tokenizer", None) is not None:
                    self.tokenizer.save_pretrained(output_dir)
                # 4) é¢å¤–å†™ä¸ªå°æ ‡è®°ï¼Œæ–¹ä¾¿åŠ è½½æ—¶è¯†åˆ«
                with open(os.path.join(output_dir, "value_head_config.json"), "w") as f:
                    f.write("{}")
            else:
                # å›é€€åˆ°çˆ¶ç±»é»˜è®¤ä¿å­˜
                super()._save(output_dir, state_dict=state_dict)
    return EvalPairwiseTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_test,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )


# --- ä¸»ç¨‹åºæ‰§è¡Œ ---

def parse_args() -> ScriptArguments:
    parser = argparse.ArgumentParser(description="å¥–åŠ±æ¨¡å‹è®­ç»ƒè„šæœ¬")
    parser.add_argument("--model", dest="model_name", type=str, default=DEFAULT_MODEL_NAME)
    parser.add_argument("--dataset", dest="dataset_name", type=str, default=DEFAULT_DATASET)
    parser.add_argument("--output", dest="output_dir", type=str, default="./outputs/reward_model")
    parser.add_argument("--lr", dest="learning_rate", type=float, default=5e-6)
    parser.add_argument("--epochs", dest="num_train_epochs", type=int, default=1)
    parser.add_argument("--batch", dest="per_device_batch_size", type=int, default=2)
    parser.add_argument("--max-length", dest="max_length", type=int, default=512)
    parser.add_argument("--frac", dest="fraction", type=float, default=0.001) # æ–°å¢ fraction å‚æ•°
    parsed = parser.parse_args()
    return ScriptArguments(**vars(parsed))


if __name__ == "__main__":
    args = parse_args()
    
    # ç¡®ä¿å®‰è£…äº†éœ€è¦çš„åº“
    print("ğŸš€ æ£€æŸ¥ä¾èµ–ï¼šè¯·ç¡®ä¿å·²å®‰è£… transformers, datasets, evaluate, numpy, torch")
    
    # 1. åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right" 
    
    # 2. åŠ è½½æ•°æ®é›†
    print(f"ğŸ§© æ­£åœ¨åŠ è½½æ•°æ®é›†: {args.dataset_name} (æ¯”ä¾‹: {args.fraction})")
    ds_train, ds_test = load_preference_dataset(args)
    print(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(ds_train)}, æµ‹è¯•é›†å¤§å°: {len(ds_test)}")
    
    # 3. æ•°æ®é¢„å¤„ç†
    print("ğŸ“ æ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†å’Œ Tokenization...")
    tokenized_train = preprocess(ds_train, tokenizer, args.max_length)
    tokenized_test  = preprocess(ds_test, tokenizer, args.max_length)
    
    # 4. å‡†å¤‡ Trainer
    print(f"ğŸ§  æ­£åœ¨å‡†å¤‡ Trainer, ä½¿ç”¨æ¨¡å‹: {args.model_name}")
    trainer = make_trainer(args, tokenized_train, tokenized_test, tokenizer)
    
    # 5. å¼€å§‹è®­ç»ƒ
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒå¥–åŠ±æ¨¡å‹...")
    trainer.train()
    print("âœ… è®­ç»ƒå®Œæˆã€‚")