# 教程：对齐过程中的偏差检测

## 学习目标
- 构建敏感话题探针，检测模型回答中的潜在偏见与负面表述。
- 学会结合关键词搜索与情感分析生成结构化报告。
- 理解偏差检测结果的解读方式，为模型伦理评估提供依据。

## 背景原理
偏差检测通常包含两部分：
1. **规则检测**：通过敏感主题列表与负面关键词识别明显违规内容。
2. **情感分析**：评估输出的情绪倾向，若出现负向情感需进一步审查。

## 代码结构解析
- `BiasArguments`：定义模型名称、报告输出目录与自定义提示语文件。
- `load_prompts`：加载默认探针或用户提供的提示语列表。
- `generate_responses`：调用模型生成回答，封装为 `DataFrame`。
- `detect_bias`：使用关键词匹配与情感分析模型（如 `uer/roberta-base-finetuned-jd-binary-chinese`）打标。
- `save_report`：将结果导出为 CSV 与 Markdown，便于分享与归档。

## 实践步骤
1. 根据业务场景编写探针问题，保存到文本文件，通过 `--prompts` 指定。
2. 运行脚本：
   ```bash
   python bias_scan.py --model Qwen/Qwen3-1.8B-Instruct --output outputs/bias_report
   ```
3. 查看生成的 `bias_report.md`，分析负面关键词与敏感主题触发情况。
4. 将问题案例反馈给数据标注或模型优化团队，制定修复策略。

## 拓展问题
- 如何引入更细粒度的分类器（如仇恨言论、多类别情感）提升检测覆盖度？
- 对于多轮对话，是否需要考虑上下文关联才能准确判断偏差？
- 可否结合 RLHF 奖励模型，对偏差样本给予负向奖励实现闭环优化？
