# -*- coding: utf-8 -*-
"""
å¤šå¡ DPO è®­ç»ƒè„šæœ¬ (åŸºäº TRL + Accelerate)
====================================================
- æ”¯æŒå•æœºå¤š GPU å¹¶è¡Œ
- è‡ªåŠ¨åˆ†å¸ƒå¼åŒæ­¥æ¢¯åº¦
- å¯é€‰ AMP / ZeRO ä¼˜åŒ–
- é€‚é… Qwen3 ç³»åˆ—æ¨¡å‹ï¼ˆattn_implementation='eager'ï¼‰

è¿è¡Œæ–¹å¼:
torchrun --nproc_per_node=4 train_dpo_trl_multigpu.py
æˆ–
accelerate launch train_dpo_trl_multigpu.py
"""

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from trl import DPOTrainer, DPOConfig
import torch
import os

# =====================================================
# 1ï¸âƒ£ ç¯å¢ƒå‡†å¤‡
# =====================================================
os.environ.setdefault("PYTORCH_USE_FLASH_ATTENTION", "0")  # é˜²æ­¢ FA2 å†²çª
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

# =====================================================
# 2ï¸âƒ£ åŠ è½½æ•°æ® (ç¤ºä¾‹ï¼šä»… 0.1%)
# =====================================================
dataset = load_dataset("lvwerra/stack-exchange-paired", split="train", data_dir="data/finetune")
dataset = dataset.select(range(int(len(dataset) * 0.001)))

# æ•°æ®æ ¼å¼ï¼š
# question, response_j (chosen), response_k (rejected)

# =====================================================
# 3ï¸âƒ£ åŠ è½½ tokenizer
# =====================================================
model_name = "Qwen/Qwen3-0.6b"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# =====================================================
# 4ï¸âƒ£ æ„å»º DPO Trainer (TRL)
# =====================================================
# DPO åŸç†ï¼šmax E[ log Ï€_Î¸(yâº|x) âˆ’ log Ï€_Î¸(yâ»|x) ]ï¼Œä»¥å‚è€ƒæ¨¡å‹ä¸ºå¯¹ç…§
#           Î² æ§åˆ¶åå¥½å¼ºåº¦ï¼Œè¶Šå¤§è¶Šæ¥è¿‘ç›´æ¥å¯¹æ¯” KLã€‚
# TRL çš„ DPOTrainer è‡ªåŠ¨å¤„ç†æ•°æ®å¹¶è¡Œã€æ¢¯åº¦åŒæ­¥ç­‰ã€‚

# åˆå§‹åŒ– policy/reference æ¨¡å‹
policy_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation="eager",
    trust_remote_code=True,
)
ref_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation="eager",
    trust_remote_code=True,
)
ref_model.requires_grad_(False)

# =====================================================
# 5ï¸âƒ£ é…ç½® DPO è¶…å‚æ•°ï¼ˆå¤šå¡è‡ªåŠ¨åŒæ­¥ï¼‰
# =====================================================
dpo_config = DPOConfig(
    beta=0.1,                  # DPO å¼ºåº¦ç³»æ•°
    learning_rate=5e-6,
    max_length=512,
    max_prompt_length=256,
    max_target_length=256,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,   # å°æ˜¾å­˜åœºæ™¯
    remove_unused_columns=False,
    logging_steps=10,
    save_strategy="epoch",
    output_dir="outputs/dpo_qwen_trl",
    report_to="none",
    warmup_ratio=0.05,
    gradient_checkpointing=True,
    bf16=torch.cuda.is_bf16_supported(),
)

# =====================================================
# 6ï¸âƒ£ å¯åŠ¨ DPOTrainer
# =====================================================
trainer = DPOTrainer(
    model=policy_model,
    ref_model=ref_model,
    args=TrainingArguments(
        output_dir=dpo_config.output_dir,
        learning_rate=dpo_config.learning_rate,
        per_device_train_batch_size=dpo_config.per_device_train_batch_size,
        gradient_accumulation_steps=dpo_config.gradient_accumulation_steps,
        num_train_epochs=1,
        bf16=dpo_config.bf16,
        logging_steps=10,
        save_strategy="epoch",
        report_to="none",
    ),
    beta=dpo_config.beta,
    train_dataset=dataset,
    tokenizer=tokenizer,
    max_length=dpo_config.max_length,
    max_prompt_length=dpo_config.max_prompt_length,
    max_target_length=dpo_config.max_target_length,
)

# =====================================================
# 7ï¸âƒ£ å¯åŠ¨è®­ç»ƒï¼ˆå¤šå¡è‡ªåŠ¨å¹¶è¡Œï¼‰
# =====================================================
print("ğŸš€ Starting multi-GPU DPO training ...")
trainer.train()
print("âœ… Training complete.")

# =====================================================
# 8ï¸âƒ£ ä¿å­˜æ¨¡å‹
# =====================================================
output_dir = dpo_config.output_dir
print(f"âœ… Saving final model to {output_dir}")
trainer.model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
