# 教程：Qwen 对齐流程解析与 PPO 演示

## 学习目标
- 熟悉 Qwen 模型在 RLHF 流程中的关键组件：数据加载、奖励模型、PPO 策略更新。
- 学会使用 TRL 库快速搭建价值头模型与 PPO 训练循环。
- 理解如何通过命令行参数控制示例规模，便于课堂演示。

## 背景原理
RLHF 通常包含三个步骤：
1. **收集偏好数据**：包含 prompt、优选回答与被拒回答。
2. **训练奖励模型**：估计回答质量分数，指导策略学习。
3. **策略优化（PPO）**：在奖励信号与 KL 约束下更新模型权重。

## 代码结构解析
- `WorkflowArguments`：集中管理模型名称、数据集、学习率、PPO 步数等参数。
- `load_preference_data`：加载并采样偏好数据集，兼顾训练与调试子集。
- `train_reward_model`：构建带价值头的模型并示例性计算分数，课堂中可替换为完整奖励模型训练。
- `run_ppo_alignment`：初始化策略与参考模型，运行简化的 PPO 循环并保存对齐后的权重。
- `parse_args` / `main`：提供命令行入口，串联整个流程。

## 实践步骤
1. 准备偏好数据集，调整 `--sample` 控制采样比例以适应算力。
2. 首次演示可保留脚本中的示例奖励模型逻辑，强调价值头作用；进阶可替换为 Lesson 4 完整训练产物。
3. 观察 `trainer.step` 返回的 KL 与 reward 指标，讨论超参数对稳定性的影响。
4. 训练结束后，加载 `outputs/qwen_ppo_aligned` 在对话任务中检验效果。

## 拓展讨论
- 如何在奖励模型中引入多维度信号（安全、礼貌）并组合成单一分数？
- 若算力有限，可否改用 DPO 等无强化学习方法快速对齐？
- PPO 训练中如何设置 `target_kl` 与 `learning_rate`，避免过大梯度导致崩溃？
