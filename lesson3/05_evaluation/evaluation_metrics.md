# 教程：困惑度计算与人工评估模板

## 学习目标
- 掌握使用语言模型输出计算困惑度（Perplexity, PPL）的方法。
- 学会生成人工评估表格，用于主观质量打分。
- 理解自动指标与人工评估之间的互补关系。

## 背景原理
困惑度衡量模型对测试集的预测能力：
\[
\text{PPL} = \exp\left( -\frac{1}{N} \sum_{i=1}^{N} \log p_{\theta}(w_i \mid w_{<i}) \right).
\]
数值越低表示模型对数据越“自信”。然而自动指标难以覆盖礼貌性、事实性等维度，因此仍需人工评估。

## 代码结构解析
- `EvalConfig`：定义模型名称与待测文本列表。
- `compute_ppl`：对每条文本执行前向计算，累积损失并求指数得到 PPL。
- `build_human_eval_template`：构造含有 10 条案例的人工评估 CSV 模板。
- `main`：导出模板并尝试计算困惑度，若无模型则提示跳过。

## 实操步骤
1. 将 `texts` 替换为实际验证集样本。
2. 运行脚本生成 `human_eval_template.csv`，发给标注同学填写评分与备注。
3. 若本地具备模型权重，脚本会输出困惑度，可与微调前结果对比。
4. 综合 PPL 与人工评分，判断模型优化是否达成目标。

## 进一步思考
- 对长文本可使用滑动窗口或分句计算困惑度，避免显存不足。
- 人工评估表格可加入“安全性”“事实性”等维度，实现多指标评分。
- 如何将人工评分转化为奖励信号，用于 RLHF 或偏好模型训练？
