{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ac9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Jupyter Notebook 版本：多轮对话 + KV 缓存 + Markdown/LaTeX 渲染\n",
    "- 用 IPython.display.Markdown 渲染答案（支持 MathJax 公式）\n",
    "- DynamicCache 复用 KV，前缀对齐增量更新\n",
    "- 人工输入循环（在 Notebook 里 input() 可用）\n",
    "- 可选显示 <think> 内容\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import logging as hf_logging\n",
    "from transformers.cache_utils import DynamicCache\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "# 日志更安静\n",
    "hf_logging.set_verbosity_warning()\n",
    "\n",
    "# ===== 配置 =====\n",
    "model_name = \"qw1.7_model\"   # TODO: 改成你的模型\n",
    "max_new_tokens = 2048 \n",
    "use_sampling = False          # True=采样; False=贪心\n",
    "temperature = 0.7\n",
    "top_p = 0.9\n",
    "show_think = False            # True 显示 <think> 内容\n",
    "\n",
    "# ===== 加载模型/分词器 =====\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "device = model.device\n",
    "\n",
    "# 兜底 pad_token\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# ===== 初始对话历史 =====\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是‘地震学AI助手’，一个乐于助人的中文AI助手。由地震局人工智能团队开发。请帮助用户解决问题。\"}, \n",
    "]\n",
    "\n",
    "# ===== KV 缓存状态 =====\n",
    "cached_input_ids = None        # torch.LongTensor [1, L]\n",
    "kv_cache: DynamicCache | None = None\n",
    "\n",
    "def build_input_ids_from_messages(msgs):\n",
    "    \"\"\"用聊天模板把 messages 转成 token 序列\"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True  # 让模型继续生成 assistant 的回答\n",
    "    )\n",
    "    return tokenizer(prompt, return_tensors=\"pt\").to(device)[\"input_ids\"]\n",
    "\n",
    "@torch.no_grad()\n",
    "def prefill_or_update_cache(new_input_ids):\n",
    "    \"\"\"\n",
    "    复用 KV 的关键：\n",
    "    - 第一次/缓存失效：全量预填（prefill）\n",
    "    - 否则若新输入以旧输入为前缀，仅对“新增 token”做前向，增量更新 KV\n",
    "    - 不匹配则重建 KV\n",
    "    \"\"\"\n",
    "    global cached_input_ids, kv_cache\n",
    "\n",
    "    # 第一次/缓存失效\n",
    "    if cached_input_ids is None or kv_cache is None:\n",
    "        kv_cache = DynamicCache()\n",
    "        _ = model(input_ids=new_input_ids, use_cache=True, past_key_values=kv_cache)\n",
    "        cached_input_ids = new_input_ids\n",
    "        last_logits = model(input_ids=new_input_ids[:, -1:], use_cache=True, past_key_values=kv_cache).logits[:, -1, :]\n",
    "        return last_logits\n",
    "\n",
    "    old_len = cached_input_ids.shape[1]\n",
    "    # 前缀对齐：只跑新增 token\n",
    "    if new_input_ids.shape[1] >= old_len and torch.equal(new_input_ids[:, :old_len], cached_input_ids):\n",
    "        delta = new_input_ids[:, old_len:]\n",
    "        if delta.numel() > 0:\n",
    "            _ = model(input_ids=delta, use_cache=True, past_key_values=kv_cache)\n",
    "            cached_input_ids = new_input_ids\n",
    "        last_logits = model(input_ids=new_input_ids[:, -1:], use_cache=True, past_key_values=kv_cache).logits[:, -1, :]\n",
    "        return last_logits\n",
    "\n",
    "    # 不匹配：重建 KV\n",
    "    kv_cache = DynamicCache()\n",
    "    _ = model(input_ids=new_input_ids, use_cache=True, past_key_values=kv_cache)\n",
    "    cached_input_ids = new_input_ids\n",
    "    last_logits = model(input_ids=new_input_ids[:, -1:], use_cache=True, past_key_values=kv_cache).logits[:, -1, :]\n",
    "    return last_logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_with_kv(last_logits, max_tokens=512, eos_id=None):\n",
    "    \"\"\"\n",
    "    显式逐 token 生成，完全依赖 KV（每步只喂一个新 token）\n",
    "    可切换采样/贪心；避免使用 model.generate 产生参数兼容性日志\n",
    "    \"\"\"\n",
    "    global kv_cache\n",
    "    generated_ids = []\n",
    "    cur_logits = last_logits\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        if use_sampling:\n",
    "            probs = torch.softmax(cur_logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)  # [1,1]\n",
    "        else:\n",
    "            next_id = torch.argmax(cur_logits, dim=-1, keepdim=True)  # [1,1]\n",
    "\n",
    "        tid = next_id.item()\n",
    "        generated_ids.append(tid)\n",
    "\n",
    "        if eos_id is not None and tid == eos_id:\n",
    "            break\n",
    "\n",
    "        # 增量一步：只喂入新 token，更新 KV 并取下一个位置 logits\n",
    "        cur_logits = model(input_ids=next_id, use_cache=True, past_key_values=kv_cache).logits[:, -1, :]\n",
    "\n",
    "    if len(generated_ids) == 0:\n",
    "        return torch.empty((1, 0), dtype=torch.long, device=device)\n",
    "    return torch.tensor(generated_ids, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def split_think(text: str):\n",
    "    \"\"\"抽出 <think> 内容，并返回外层文本\"\"\"\n",
    "    insides = re.findall(r\"<think>(.*?)</think>\", text, flags=re.S)\n",
    "    outside = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.S).strip()\n",
    "    return insides, outside\n",
    "\n",
    "def render_markdown(md_text: str, title: str | None = None):\n",
    "    \"\"\"\n",
    "    在 Jupyter 中用 Markdown 渲染文本（自动支持 LaTeX 公式）\n",
    "    - 行内：$...$\n",
    "    - 块级：$$...$$\n",
    "    \"\"\"\n",
    "    if title:\n",
    "        display(Markdown(f\"### {title}\"))\n",
    "    display(Markdown(md_text))\n",
    "\n",
    "# ======== 首轮对话（演示） ========\n",
    "#input_ids = build_input_ids_from_messages(messages)\n",
    "#last_logits = prefill_or_update_cache(input_ids)\n",
    "#gen_ids = generate_with_kv(last_logits, max_tokens=max_new_tokens, eos_id=tokenizer.eos_token_id)\n",
    "#\n",
    "#full_ids = torch.cat([input_ids, gen_ids], dim=1)\n",
    "#text = tokenizer.decode(full_ids[0], skip_special_tokens=True)\n",
    "#\n",
    "#think_spans, answer = split_think(text)\n",
    "#final_answer = answer.split(\"assistant\")[-1].strip()\n",
    "#\n",
    "#if show_think and think_spans:\n",
    "#    display(Markdown(\"**思考过程 `<think>`：**\"))\n",
    "#    display(Markdown(f\"```\\n{think_spans[-1].strip()}\\n```\"))\n",
    "#\n",
    "#render_markdown(final_answer, title=\"助手回复（Markdown/LaTeX 渲染）\")\n",
    "#\n",
    "## 写回历史 + 并入缓存\n",
    "#messages.append({\"role\": \"assistant\", \"content\": final_answer})\n",
    "#cached_input_ids = full_ids\n",
    "\n",
    "# ======== 多轮对话循环（在 Notebook 中人工输入） ========\n",
    "print(\"进入多轮对话（输入 q 退出）\")\n",
    "while True:\n",
    "    try:\n",
    "        user_text = input(\"\\n你：\").strip()\n",
    "        if user_text.lower() in {\"q\", \"quit\", \"exit\"}:\n",
    "            print(\"已退出。\")\n",
    "            break\n",
    "        if not user_text:\n",
    "            continue\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "        # 构造输入 + 复用 KV\n",
    "        input_ids = build_input_ids_from_messages(messages)\n",
    "        last_logits = prefill_or_update_cache(input_ids)\n",
    "        gen_ids = generate_with_kv(last_logits, max_tokens=max_new_tokens, eos_id=tokenizer.eos_token_id)\n",
    "\n",
    "        # 解码 + 渲染\n",
    "        full_ids = torch.cat([input_ids, gen_ids], dim=1)\n",
    "        text = tokenizer.decode(full_ids[0], skip_special_tokens=True)\n",
    "        think_spans, answer = split_think(text)\n",
    "        final_answer = answer.split(\"assistant\")[-1].strip()\n",
    "\n",
    "        if show_think and think_spans:\n",
    "            display(Markdown(\"**思考过程 `<think>`：**\"))\n",
    "            display(Markdown(f\"```\\n{think_spans[-1].strip()}\\n```\"))\n",
    "\n",
    "        render_markdown(final_answer, title=\"助手回复\")\n",
    "\n",
    "        # 写回历史 + 并入缓存\n",
    "        messages.append({\"role\": \"assistant\", \"content\": final_answer})\n",
    "        cached_input_ids = torch.cat([input_ids, gen_ids], dim=1)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n已退出。\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff280588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bac5ee88b274dd388d3a6301ae007e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进入多轮对话（在下方输入，'q' 退出）\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "好的，用户让我解释地震面波。首先，我需要确定用户对地震学的基础知识了解多少。可能他们之前接触过地震波的基本分类，比如体波和面波，但需要更详细的解释。或者他们可能完全不了解，需要从头开始讲起。\n",
       "\n",
       "接下来，我应该先定义什么是地震面波。面波是沿着地球表面传播的地震波，和体波不同，体波是在地球内部传播的。面波通常在地震发生后到达，速度比体波慢，但振幅更大，所以破坏力更强。这一点很重要，因为用户可能想知道为什么面波更危险。\n",
       "\n",
       "然后，我需要分类型讲解。面波主要有两种：瑞利波和洛夫波。瑞利波的运动方式像海浪，既有垂直也有水平运动，而洛夫波主要是水平方向的剪切运动。这部分需要详细说明两者的区别，可能用户会混淆这两种波的运动方式。\n",
       "\n",
       "还要提到面波的传播路径和速度。面波只在地表附近传播，速度比体波慢，所以到达时间更晚。但因为它们的能量集中在地表，所以对建筑物的破坏更大。这里可能需要举例说明，比如在地震中，面波导致的摇晃更持久，更容易造成结构损坏。\n",
       "\n",
       "另外，用户可能想知道面波在地震学中的应用。比如，面波用于研究地球内部结构，或者在地震预警系统中的作用。这部分可以简要提及，但不需要太深入，除非用户进一步询问。\n",
       "\n",
       "还要注意术语的解释是否清晰，比如“振幅”、“剪切运动”等，可能需要用更通俗的语言。同时，检查是否有常见的误解需要澄清，比如面波是否总是比体波慢，或者是否所有地震都有面波。\n",
       "\n",
       "最后，确保回答结构清晰，分点说明，让用户容易理解。可能需要先总述面波的定义和特点，再分类型详细解释，最后总结其影响和应用。同时，保持语言简洁，避免过于专业的术语，除非用户有相关背景。\n",
       "</think>\n",
       "\n",
       "地震面波是地震波的一种，主要沿地球表面传播，是地震发生后到达地表的次生波。与体波（如P波和S波）不同，面波的能量集中在地表附近，因此对地表建筑物和人类活动的影响更为显著。以下是关于地震面波的详细解释：\n",
       "\n",
       "---\n",
       "\n",
       "### **1. 面波的类型**\n",
       "面波主要分为两种类型：\n",
       "- **瑞利波（Rayleigh Wave）**  \n",
       "  - **运动方式**：类似海浪的滚动运动，既有垂直方向的上下振动，也有水平方向的前后摆动。  \n",
       "  - **特点**：振幅较大，传播速度较慢（约2.0-4.5 km/s），是地震中常见的破坏性波。  \n",
       "  - **破坏性**：由于其复杂的运动模式，容易引发建筑物的共振，导致更严重的破坏。\n",
       "\n",
       "- **洛夫波（Love Wave）**  \n",
       "  - **运动方式**：仅在水平方向上横向剪切运动（类似S波的剪切效应）。  \n",
       "  - **特点**：传播速度略快于瑞利波（约2.5-5.0 km/s），但振幅通常较小。  \n",
       "  - **破坏性**：对地表结构（如桥梁、管道）的水平剪切破坏较强。\n",
       "\n",
       "---\n",
       "\n",
       "### **2. 面波的传播特性**\n",
       "- **传播路径**：面波仅沿地球表面传播，能量集中在地表几公里范围内。  \n",
       "- **速度**：比体波（P波和S波）慢，因此在地震中"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Jupyter Notebook：多轮对话 + KV 缓存 + Markdown/LaTeX 流式渲染\n",
    "- 人工输入多轮对话\n",
    "- DynamicCache 复用 KV（前缀对齐增量预填）\n",
    "- 每生成一部分 token 就流式更新显示（MathJax 渲染公式）\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import logging as hf_logging\n",
    "from transformers.cache_utils import DynamicCache\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "hf_logging.set_verbosity_warning()\n",
    "\n",
    "# ===== 配置 =====\n",
    "model_name = \"qw32_model\"     # ← 改成你的模型名称或本地路径\n",
    "max_new_tokens = 2048 \n",
    "stream_interval = 8            # 每多少个 token 刷新一次\n",
    "use_sampling = False           # True=采样；False=贪心\n",
    "temperature = 0.7\n",
    "top_p = 0.9\n",
    "show_think = False             # 是否显示 <think> 内容\n",
    "\n",
    "# ===== 加载模型/分词器 =====\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "device = model.device\n",
    "\n",
    "# 兜底 pad_token\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# ===== 对话历史（可根据需要预置开场） =====\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是‘地震学AI助手’，一个乐于助人的中文AI助手。由地震局人工智能团队开发。请帮助用户解决问题。\"},\n",
    "]\n",
    "\n",
    "# ===== KV 缓存状态 =====\n",
    "cached_input_ids = None        # torch.LongTensor [1, L]\n",
    "kv_cache: DynamicCache | None = None\n",
    "\n",
    "def build_input_ids_from_messages(msgs):\n",
    "    \"\"\"将 messages 用聊天模板转成 token 序列\"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return tokenizer(prompt, return_tensors=\"pt\").to(device)[\"input_ids\"]\n",
    "\n",
    "@torch.no_grad()\n",
    "def prefill_or_update_cache(new_input_ids):\n",
    "    \"\"\"\n",
    "    复用 KV 的关键：\n",
    "    - 第一次/缓存失效：全量预填（prefill）\n",
    "    - 否则若新输入以旧输入为前缀：仅对“新增 token”做前向，增量更新 KV\n",
    "    - 不匹配则重建 KV\n",
    "    \"\"\"\n",
    "    global cached_input_ids, kv_cache\n",
    "\n",
    "    # 第一次/缓存失效\n",
    "    if cached_input_ids is None or kv_cache is None:\n",
    "        kv_cache = DynamicCache()\n",
    "        _ = model(input_ids=new_input_ids, use_cache=True, past_key_values=kv_cache)\n",
    "        cached_input_ids = new_input_ids\n",
    "        last_logits = model(input_ids=new_input_ids[:, -1:], use_cache=True, past_key_values=kv_cache).logits[:, -1, :]\n",
    "        return last_logits\n",
    "\n",
    "    old_len = cached_input_ids.shape[1]\n",
    "    # 前缀对齐：只跑新增 token\n",
    "    if new_input_ids.shape[1] >= old_len and torch.equal(new_input_ids[:, :old_len], cached_input_ids):\n",
    "        delta = new_input_ids[:, old_len:]\n",
    "        if delta.numel() > 0:\n",
    "            _ = model(input_ids=delta, use_cache=True, past_key_values=kv_cache)\n",
    "            cached_input_ids = new_input_ids\n",
    "        last_logits = model(input_ids=new_input_ids[:, -1:], use_cache=True, past_key_values=kv_cache).logits[:, -1, :]\n",
    "        return last_logits\n",
    "\n",
    "    # 不匹配：重建 KV\n",
    "    kv_cache = DynamicCache()\n",
    "    _ = model(input_ids=new_input_ids, use_cache=True, past_key_values=kv_cache)\n",
    "    cached_input_ids = new_input_ids\n",
    "    last_logits = model(input_ids=new_input_ids[:, -1:], use_cache=True, past_key_values=kv_cache).logits[:, -1, :]\n",
    "    return last_logits\n",
    "\n",
    "def split_think(text: str):\n",
    "    \"\"\"抽取 <think> 内容，并返回外层文本\"\"\"\n",
    "    insides = re.findall(r\"<think>(.*?)</think>\", text, flags=re.S)\n",
    "    outside = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.S).strip()\n",
    "    return insides, outside\n",
    "\n",
    "@torch.no_grad()\n",
    "def stream_generate(last_logits, eos_id=None, max_tokens=512, base_prompt_ids=None, display_id=None):\n",
    "    \"\"\"\n",
    "    逐 token 生成 + 流式渲染（在 Notebook 中持续更新同一个输出块）\n",
    "    - base_prompt_ids: prompt 的 token（用于 decode 完整上下文避免分词裂缝）\n",
    "    - display_id: Notebook display_id（复用同一个显示块）\n",
    "    返回：本轮生成的 token ids (tensor [1, T])\n",
    "    \"\"\"\n",
    "    global kv_cache\n",
    "    generated_ids = []\n",
    "    cur_logits = last_logits\n",
    "\n",
    "    # 用 display_id 创建/获取一个输出块\n",
    "    if display_id is None:\n",
    "        handle = display(Markdown(\"\"), display_id=True)\n",
    "    else:\n",
    "        handle = display(Markdown(\"\"), display_id=display_id)\n",
    "\n",
    "    for step in range(max_tokens):\n",
    "        # 选 token\n",
    "        if use_sampling:\n",
    "            probs = torch.softmax(cur_logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_id = torch.argmax(cur_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        tid = next_id.item()\n",
    "        generated_ids.append(tid)\n",
    "\n",
    "        # 结束符\n",
    "        if eos_id is not None and tid == eos_id:\n",
    "            break\n",
    "\n",
    "        # 每隔若干 token 刷新一次（把上下文+已生成一起 decode，保证 Markdown/公式完整性尽量好）\n",
    "        if (step + 1) % stream_interval == 0:\n",
    "            if base_prompt_ids is not None:\n",
    "                full_ids = torch.cat([base_prompt_ids, torch.tensor(generated_ids, device=device).view(1, -1)], dim=1)\n",
    "                text = tokenizer.decode(full_ids[0], skip_special_tokens=True)\n",
    "            else:\n",
    "                text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            _, outside = split_think(text)\n",
    "            partial_answer = outside.split(\"assistant\")[-1].strip()\n",
    "            handle.update(Markdown(partial_answer))\n",
    "\n",
    "        # 增量一步：只喂入新 token，更新 KV，并取下一个位置 logits\n",
    "        cur_logits = model(input_ids=next_id, use_cache=True, past_key_values=kv_cache).logits[:, -1, :]\n",
    "\n",
    "    # 最终完整输出\n",
    "    if generated_ids:\n",
    "        if base_prompt_ids is not None:\n",
    "            full_ids = torch.cat([base_prompt_ids, torch.tensor(generated_ids, device=device).view(1, -1)], dim=1)\n",
    "            text = tokenizer.decode(full_ids[0], skip_special_tokens=True)\n",
    "        else:\n",
    "            text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        think_spans, outside = split_think(text)\n",
    "        final_answer = outside.split(\"assistant\")[-1].strip()\n",
    "\n",
    "        if show_think and think_spans:\n",
    "            # 先显示 think，再显示最终答案\n",
    "            handle.update(Markdown(\"**思考过程 `<think>`：**\\n\\n```\\n\" + think_spans[-1].strip() + \"\\n```\"))\n",
    "            handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "        handle.update(Markdown(final_answer))\n",
    "\n",
    "    if len(generated_ids) == 0:\n",
    "        return torch.empty((1, 0), dtype=torch.long, device=device)\n",
    "    return torch.tensor(generated_ids, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "# ===== 多轮对话循环 =====\n",
    "print(\"进入多轮对话（在下方输入，'q' 退出）\")\n",
    "while True:\n",
    "    try:\n",
    "        user_text = input(\"\\n你：\").strip()\n",
    "        if user_text.lower() in {\"q\", \"quit\", \"exit\"}:\n",
    "            print(\"已退出。\")\n",
    "            break\n",
    "        if not user_text:\n",
    "            continue\n",
    "\n",
    "        # 追加用户消息\n",
    "        messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "        # 构造输入 + 复用 KV（仅增量预填）\n",
    "        input_ids = build_input_ids_from_messages(messages)\n",
    "        last_logits = prefill_or_update_cache(input_ids)\n",
    "\n",
    "        # 启动流式生成：把当前 prompt ids 传入，用于流式 decode 完整上下文\n",
    "        gen_ids = stream_generate(\n",
    "            last_logits,\n",
    "            eos_id=tokenizer.eos_token_id,\n",
    "            max_tokens=max_new_tokens,\n",
    "            base_prompt_ids=input_ids,\n",
    "        )\n",
    "\n",
    "        # 取最终答案文本（用于写回历史 & 下轮前缀）\n",
    "        full_ids = torch.cat([input_ids, gen_ids], dim=1)\n",
    "        text = tokenizer.decode(full_ids[0], skip_special_tokens=True)\n",
    "        _, outside = split_think(text)\n",
    "        final_answer = outside.split(\"assistant\")[-1].strip()\n",
    "\n",
    "        # 写回历史；合并到缓存前缀\n",
    "        messages.append({\"role\": \"assistant\", \"content\": final_answer})\n",
    "        cached_input_ids = full_ids\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n已退出。\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
