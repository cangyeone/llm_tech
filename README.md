# LLM Tech 课程实验总览

本仓库汇集了「LLM Tech」系列课程的全部实验脚本与配套资料，按照课程节次划分为七大模块，覆盖监督微调、参数高效微调、分布式训练、模型对齐、落地监控以及 RAG 检索增强应用等核心主题。每个子目录都提供结构化的代码示例与中文注释，便于在课堂上演示或课后复现。

## 课程结构与学习要点

### Lesson 1 · 监督与指令微调基础
- **学习目标**：理解监督微调（SFT）的流程、指令数据构建方法，以及 LoRA 在轻量模型上的应用与部署策略。
- **实践内容**：涵盖 SFT 理论解析、数据清洗与 Prompt 模板构建、Hugging Face Trainer 脚手架、LoRA 参数可视化与 ChatGLM 微调示例，以及 CPU 环境的推理部署建议。【F:lesson1/README.md†L1-L29】

### Lesson 2 · QLoRA、P-Tuning 与模型压缩
- **学习目标**：掌握 QLoRA 的量化思路、端到端微调流程，理解 P-Tuning v2 的提示学习范式，并熟悉多种模型压缩与部署策略的取舍。
- **实践内容**：提供 QLoRA 理论脚本、LLaMA 量化微调管线、P-Tuning v2 可学习提示示例、剪枝/蒸馏/量化对比实验、DeepSpeed 量化部署以及方法选择助手。【F:lesson2/README.md†L1-L27】

### Lesson 3 · 分布式训练与性能调优
- **学习目标**：构建对 ZeRO-3、混合精度与大规模分布式训练的整体认知，掌握训练加速技巧、日志诊断与评估指标体系。
- **实践内容**：包含 ZeRO-3 配置生成、8 卡训练配置模板、FlashAttention 与内存优化示例、训练日志分析工具、困惑度与人工评估脚本，以及垂直领域微调案例。【F:lesson3/README.md†L1-L33】

### Lesson 4 · 大模型对齐与增量学习
- **学习目标**：系统梳理 RLHF、DPO、KTO 等主流对齐技术，掌握奖励模型训练、增量学习与对齐数据构建方法。
- **实践内容**：对齐方法对比脚本、完整 RLHF 流程、偏好数据奖励模型训练、DPO 训练示例、增量 LoRA 策略以及对齐数据集质量分析工具。【F:lesson4/README.md†L1-L27】

### Lesson 5 · 对齐系统落地与监控
- **学习目标**：了解在真实业务中部署对齐模型的流程，包括平台化 DPO、偏差检测、安全合规与模型版本管理。
- **实践内容**：Qwen 对齐流程演练、Hugging Face 平台 DPO 复现、偏差与安全审查脚本、MLflow/W&B 版本管理、客服场景策略以及回答多样性调试工具。【F:lesson5/README.md†L1-L24】

### Lesson 6 · 知识库与 RAG 实战
- **学习目标**：搭建检索增强生成（RAG）系统，掌握向量库选型、文档预处理、架构搭建、本地化部署与效果优化流程。
- **实践内容**：比较常见向量库、完成文档分块与向量化、构建 FAISS 向量库、解析 RAG 组件协同、部署 RAGFlow 以及监控检索与生成指标的调优实验。【F:lesson6/README.md†L1-L23】

### Lesson 7 · 知识库优化与混合推理
- **学习目标**：面向企业级知识库场景，掌握混合检索、流程编排、增量更新、混合推理与性能压测的最佳实践。
- **实践内容**：实现 BM25+向量混合检索、基于 LangChain 的 RAG 编排、增量索引策略、企业客服案例拆解、对齐模型与 RAG 的混合推理，以及检索/生成性能压测工具。【F:lesson7/README.md†L1-L23】

## 使用建议
- 建议先阅读各课 `README`，根据自身算力与业务场景选择合适的示例脚本与模型规模。
- 大部分脚本支持通过命令行参数或配置文件调整模型名称、批大小、向量库连接等关键参数，可灵活适配不同环境。
- 若需扩展到生产环境，可结合 Lesson 4-7 的对齐与 RAG 工具链，搭建端到端的企业级问答与知识服务体系。

