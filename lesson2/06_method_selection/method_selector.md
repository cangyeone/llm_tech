# 教程：LoRA/QLoRA/P-Tuning 场景选择指南

## 学习目标
- 根据硬件资源、数据规模与延迟要求选择合适的微调策略。
- 通过程序化逻辑快速给出部署建议，辅助团队决策。
- 结合课程案例分析不同场景下的优化手段。

## 背景原理
微调策略选择通常遵循以下准则：
- **硬件限制**：CPU 或单卡 GPU 更适合参数高效微调（PEFT），多卡环境可以考虑全量微调或分布式 QLoRA。
- **数据规模**：大规模数据需要更强的表示能力与稳定训练技巧，如 4 bit 量化 + 梯度检查点。
- **延迟需求**：对实时性要求高时，需兼顾推理性能，可结合蒸馏或量化部署。

## 代码结构解析
- `Scenario`：通过 `@dataclass` 封装硬件、数据规模、延迟等级与描述。
- `recommend`：根据场景分支返回建议，例如 LoRA、QLoRA 或 P-Tuning。
- `__main__`：提供默认示例，方便运行时直接观察输出。

## 使用方法
1. 根据实际情况构造 `Scenario` 对象，例如：
   ```python
   Scenario("multi_gpu", "large", "strict", "企业客服大模型上线")
   ```
2. 调用 `recommend` 获取建议，并在文档中补充说明原因。
3. 可扩展函数逻辑，引入更多维度（如预算、推理平台）以给出更细分的方案。

## 深入思考
- 如何量化“数据规模”与“延迟要求”，可否结合历史实验结果形成规则表？
- 若团队拥有自动化调度系统，可否将该函数嵌入到流水线中动态选择策略？
- 当模型出现灾难性遗忘风险时，是否需要在建议中加入增量学习方案？
