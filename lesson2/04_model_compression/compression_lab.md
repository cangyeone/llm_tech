# 教程：模型剪枝、蒸馏与低比特量化实验

## 学习目标
- 理解注意力头剪枝对模型结构与参数的影响。
- 掌握教师-学生蒸馏的基本训练流程与损失函数选择。
- 模拟 INT8 量化误差，认识低比特量化对权重的近似效果。

## 背景原理
1. **结构化剪枝**：通过零化部分权重实现注意力头移除，使计算复杂度按比例降低。
2. **知识蒸馏**：最小化学生输出 $S(x)$ 与教师输出 $T(x)$ 的距离：
$$
\mathcal{L}_{\text{KD}} = \lVert S(x) - T(x) \rVert_2^2.
$$
3. **量化**：将浮点权重映射到整数区间 $[q_{\min}, q_{\max}]$，通过缩放因子 $\alpha$ 实现近似恢复：$\hat{w} = \text{round}(w/\alpha) \cdot \alpha$。

## 代码结构解析
- `ToyTransformer`：简化的多头注意力 + 前馈网络，用于演示。
- `DistilledStudent`：单层学生模型，通过 `tanh` 输出拟合教师表示。
- `structured_pruning`：根据保留的注意力头数量生成掩码，实现剪枝。
- `knowledge_distillation`：使用 MSE 损失训练学生模型，返回最终损失。
- `simulate_int8_quantization`：模拟 8 bit 量化，计算权重误差范数。

## 实验步骤
1. 直接运行脚本观察日志输出：保留头数、蒸馏损失、量化误差。
2. 修改 `keep_heads`，分析剪枝率与模型表达力的权衡。
3. 将蒸馏迭代次数增加到 200，观察损失下降趋势。
4. 替换 `torch.randn` 为真实权重张量，评估实际量化误差。

## 深入探讨
- 如何根据注意力头的重要性（例如基于权重范数或注意力熵）自适应决定剪枝对象？
- 蒸馏过程中可加入温度缩放或 KL 散度损失，以更贴近实际分类任务。
- INT8 量化常配合对称/非对称方案，此处是否需要存储额外的零点参数？
